{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2287441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-kbi82kdb\n",
      "  Resolved https://github.com/huggingface/transformers to commit 5e0ffd91830f2c8a0133518d1724d6f3ff92a0e1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in e:\\anaconda3\\lib\\site-packages (from transformers==4.21.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: colorama in e:\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.21.0.dev0) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->transformers==4.21.0.dev0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->transformers==4.21.0.dev0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->transformers==4.21.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\anaconda3\\lib\\site-packages (from requests->transformers==4.21.0.dev0) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone -q https://github.com/huggingface/transformers 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-req-build-kbi82kdb'\n",
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -vxopt (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb060a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8913a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Preparing for TPU usage\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# device = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee63b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1119373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
    "loss_train = []\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        loss_train.append(loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95f2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "#             if _%100==0:\n",
    "            print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841353f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  The Administration of Union Territory Daman an...   \n",
      "1  Malaika Arora slammed an Instagram user who tr...   \n",
      "2  The Indira Gandhi Institute of Medical Science...   \n",
      "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
      "4  Hotels in Maharashtra will train their staff t...   \n",
      "\n",
      "                                               ctext  \n",
      "0  summarize: The Daman and Diu administration on...  \n",
      "1  summarize: From her special numbers to TV?appe...  \n",
      "2  summarize: The Indira Gandhi Institute of Medi...  \n",
      "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
      "4  summarize: Hotels in Mumbai and other Indian c...  \n",
      "FULL Dataset: (4514, 2)\n",
      "TRAIN Dataset: (3611, 2)\n",
      "TEST Dataset: (903, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  7.483099460601807\n",
      "Epoch: 0, Loss:  4.2015533447265625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_BATCH_SIZE = 3    # input batch size for training \n",
    "VALID_BATCH_SIZE = 3    # input batch size for testing\n",
    "TRAIN_EPOCHS = 1        # number of epochs to train \n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate \n",
    "SEED = 42               # random seed \n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150 \n",
    "\n",
    "  # Set random seeds and deterministic pytorch for reproducibility\n",
    "  torch.manual_seed(SEED) # pytorch random seed\n",
    "  np.random.seed(SEED) # numpy random seed\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  # tokenzier for encoding the text\n",
    "  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "  # Importing and Pre-Processing the domain data\n",
    "  # Selecting the needed columns only. \n",
    "  # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "  df = pd.read_csv('./datasets/news_summary.csv',encoding='latin-1')\n",
    "  df = df[['text','ctext']]\n",
    "  df.ctext = 'summarize: ' + df.ctext\n",
    "  print(df.head())\n",
    "\n",
    "\n",
    "  # Creation of Dataset and Dataloader\n",
    "  # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "  train_size = 0.8\n",
    "  train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
    "  val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "  print(\"FULL Dataset: {}\".format(df.shape))\n",
    "  print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "  print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "  training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "  val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "  # Defining the parameters for creation of dataloaders\n",
    "  train_params = {\n",
    "      'batch_size': TRAIN_BATCH_SIZE,\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0,\n",
    "      }\n",
    "\n",
    "  val_params = {\n",
    "      'batch_size': VALID_BATCH_SIZE,\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "  training_loader = DataLoader(training_set, **train_params)\n",
    "  val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "\n",
    "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "  model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "  model = model.to(device)\n",
    "\n",
    "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Training loop\n",
    "  print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "  for epoch in range(TRAIN_EPOCHS):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "  # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "  # Saving the dataframe as predictions.csv\n",
    "  print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "  for epoch in range(VAL_EPOCHS):\n",
    "      predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "      final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "      final_df.to_csv('predictions.csv')\n",
    "      print('Output Files generated for review')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e467dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
